---
title: "Group SuSiE Derivation"
author: "Andrew Goldstein"
date: "August 29, 2019"
output:
  workflowr::wflow_html:
    code_folding: show
---

# Introduction
In regular SuSiE, the response $Y \in \mathbb{R}^n$ is assumed to be normally distributed with mean $\mathbf{Xb}$ and variance $\sigma^2 I_n$. The vector of effects $\mathbf{b} \in \mathbb{R}^p$ is assumed to be the sum of $L$ "single effects":
$$
\begin{aligned}
\mathbf{b} = \sum_{l=1}^L \mathbf{b_l} \\
\mathbf{b_l} = \mathbf{\gamma_l} b_l \\
\mathbf{\gamma_l} \sim Mult(1, \mathbf{\pi}) \\
b_l \sim \mathcal{N}(0, \sigma_{ol}^2)
\end{aligned}
$$
That is, each of the $L$ components of $\mathbf{b}$ are each comprised of a single non-zero effect.

An immediate problem with this formulation is that categorical predictors do not fit into this framework. For example, in order to represents a categorical predictor with 3 levels into our design matrix $\mathbf{X}$, we require two vectors (plus the intercept), and in order for our model to handle the categorical predictor properly, both variables must be included in the model simultaneously. In fact, this exact scenario is what we wish to do in the setting where we wish to use trees of order 2 as our predictors (a direct extension of SuSiE Stumps).

In order to remedy the issue, I introduce Group SuSiE below (name pending).

# Group SuSiE
## Set-Up
In addition to our standard design matrix $\mathbf{X} \in \mathbb{R}^{n \times p}$, suppose we have $q$ pre-defined groups of predictors, with $p_j, j \in \{1, \dots, q\}$ predictors in each group. Arrange these groups of predictors into separate design matrices, $\mathbf{X^j} \in \mathbb{R}^{n \times p_j}$. Then we can define our model as follows:
$$
\begin{aligned}
Y = [X^1 \cdots X^q]
\begin{bmatrix}
\beta^1 \\
\vdots \\ \beta^q
\end{bmatrix} + \epsilon \\
\epsilon \sim \mathcal{N}(\mathbf{0}, \sigma^2 I_n) \\
\begin{bmatrix}
\beta^1 \\
\vdots \\ \beta^q
\end{bmatrix} = \sum_{l=1}^L \begin{bmatrix}
\beta_l^1 \\
\vdots \\ \beta_l^q
\end{bmatrix} := \sum_{l=1}^L \beta_l \\
\begin{bmatrix}
\beta_l^1 \\
\vdots \\ \beta_l^q
\end{bmatrix} = \gamma_l \circ \mathbf{b_l} \\
\gamma_l \sim \text{"Group-Multinomial"}(1, \pi) \\
\mathbf{b_l} \sim \mathcal{N}(\mathbf{0}, \sigma_{0l}^2 I_{p_j})
\end{aligned}
$$
where "Group-Multinomial" is a distribution that returns a binary vector with 1's in the position of the group that is selected, group $j$ being selected with probability $\pi_j$.
(Note: The notation for $\mathbf{b_l}$ is a bit weird, since in one instance it is a vector of length $\sum_{j=1}^q p_j$, and the other it is a vector of length $p_j$)

## Derivation of Updates
As in regular SuSiE, we approximate the posterior distribution using variational Bayes, where each of the $L$ effects are independent, a posteriori. Below, we derive the CAVI updates for each effect, $\beta_l$. The notation can get a bit muddied. Below, I use $\mathbf{e^j} \in \mathbb{R}^p$ to refer to the binary vector with 1's in the positions of the variables include in group $j$, I use $\mathbf{c} \in \mathbb{R}^{p_j}$ to refer to the value for $\mathbf{b_l}$, and I use $\mathbf{c^j} \in \mathbb{R}^{p_j}$ to refer to the vector where all entries corresponding to variables outside of group $j$ are 0, and the sub-vector defined by taking the entries corresponding to the variables in group $j$ is $\mathbf{c}$.

$$
\begin{aligned}
q_l(\beta_l = \mathbf{c^j} \circ \mathbf{e^j}) \propto \exp\Bigg\{\mathbb{E}_{-q_l}\Big[\log p(\beta_l = \mathbf{c^j} \circ \mathbf{e^j}) + \log p(Y|X, \beta_l = \mathbf{c^j} \circ \mathbf{e^j})\Big]\Bigg\} \propto \\
\exp\Bigg\{\log(\pi_j) - \frac{p_j}{2}\log(2\pi\sigma_{0l}^2) - \frac{1}{2} \sum_{k=1}^{p_j} \frac{\mathbf{c}_k^2}{\sigma_{0l}^2} + \sum_{i=1}^n -\frac{1}{2} \log(2\pi\sigma^2) - \frac{1}{2\sigma^2} \mathbb{E}_{-q_l}\Big[\Big(Y_i - X_i^T(\mathbf{c^j} \circ \mathbf{e^j} + \sum_{k \ne l} \beta_k)\Big)^2\Big]\Bigg\} \propto \\
\exp\Bigg\{\log(\pi_j) - \frac{p_j}{2} \log(2\pi\sigma_{ol}^2) - \frac{1}{2} \sum_{k=1}^{p_j} \frac{\mathbf{c}_k^2}{\sigma_{0l}^2} - \frac{1}{2} \sum_{i=1}^n \log(2\pi\sigma^2) + \frac{1}{\sigma^2}\Big[-2Y_i X_i^T(\mathbf{c^j} \circ \mathbf{e^j} + \sum_{k \ne l} \bar{\beta_k}) + \sum_{k=1}^{p_j} (\mathbf{c}_kX_{ik}^j)^2 + 2(\sum_{k=1}^{p_j}\mathbf{c}_k X_{ik}^j)(x_i^T\sum_{k \ne l} \bar{\beta_k})\Big]\Bigg\}  = \\
\exp\Bigg\{\log(\pi_j) - \frac{1}{2} \Big[\mathbf{c}^T(\frac{1}{\sigma_{0l}^2}I_{p_j})\mathbf{c} + \mathbf{c}^T\Big(\sum_{i=1}^n diag(\frac{X_i^{j2}}{\sigma^2})\Big)\mathbf{c} - 2\mathbf{c}^T(\sum_{i=1}^n Y_i X_i^j) + 2\mathbf{c}^T\Big(\sum_{i=1}^n (X_i^T \sum_{k \ne l} \bar{\beta_k})X_i^j\Big)\Big] - \frac{p_j}{2} \log(2\pi\sigma_{0l}^2) - \frac{n}{2} \log(2\pi\sigma^2)\Bigg\} = \\
\exp\Bigg\{\log(\pi_j) - \frac{1}{2}\Big[\mathbf{c}^Tdiag\Big(\frac{1}{\sigma_{0l}^2}\mathbf{1}_{p_j} + \frac{1}{\sigma^2} \sum_{i=1}^n X_i^{j2}\Big)\mathbf{c} - 2\mathbf{c}^T \Big(\sum_{i=1}^n (Y_i - X_i^T\sum_{k \ne l} \bar{\beta_k})X_i^j\Big)\Big] - \frac{p_j}{2} \log(2\pi\sigma_{0l}^2) - \frac{n}{2} \log(2\pi\sigma^2)\Bigg\} \propto \\
[\text{Let} \quad \bar{r}_{i, l} := Y_i - X_i^T \sum_{k \ne l} \bar{\beta_k}, \quad \tau_{k, l} := \frac{1}{\sigma_{0l}^2} + \frac{1}{\sigma^2} \sum_{i=1}^nX_{ik}^{j2}] \propto \\
\exp\Bigg\{\log(\pi_j) - \frac{1}{2} \Big[\Big(\mathbf{c} - diag(\frac{1}{\tau_l}) \sum_{i=1}^n \bar{r}_{i,l}X_i^j\Big)^T diag(\tau_l) \Big(\mathbf{c} - diag(\frac{1}{\tau_l}) \sum_{i=1}^n \bar{r}_{i,l}X_i^j\Big) - (\sum_{i=1}^n \bar{r}_{i,l} X_i^j)^T diag(\frac{1}{\tau_l})(\sum_{i=1}^n \bar{r}_{i,l} X_i^j)\Big] - \frac{p_j}{2} \log(2\pi) - \frac{p_j}{2}\log(\sigma_{0l}^2) \pm \frac{1}{2} \log(\prod_{k=1}^{p_j} \frac{1}{\tau_{k,l}})\Bigg\} = \\
[\text{Let} \quad \sigma_{k, l}^2 := \frac{1}{\tau_{k, l}}, \quad \nu_{j, l} := \sum_{i=1}^n \bar{r}_{i, l} X_i^j] = \\
\exp\Bigg\{\log(\pi_j) + \frac{1}{2} \sum_{k=1}^{p_j} \log(\frac{\sigma_{k,l}^2}{\sigma_{0l}^2}) + \frac{1}{2} \nu_{j,l}^T diag(\sigma_l^2) \nu_{j,l}\Bigg\}  \cdot \exp\Bigg\{\frac{-p_j}{2} \log(2\pi) - \frac{1}{2}\log(\prod_{k=1}^{p_j} \sigma_{k,l}^2) - \frac{1}{2} \Big[\Big(\mathbf{c} - diag(\sigma_l^2)\nu_{j,l}\Big)^T diag(\frac{1}{\sigma_l^2})\Big(\mathbf{c} - diag(\sigma_l^2)\nu_{j,l}\Big)\Big]\Bigg\} \\
\therefore \gamma_l | Y \sim \text{"Group-Multinomial"}(1, \alpha_l) \quad \text{where} \quad \alpha_{l,j} \propto \pi_j \cdot \sqrt{\prod_{k=1}^{p_j} \frac{\sigma_{k,l}^2}{\sigma_{0l}^2}} \cdot \exp\Bigg\{\frac{1}{2} \nu_{j,l}^T diag(\sigma_{l}^2)\nu_{j,l}\Bigg\} \\
\mathbf{b}_l | \gamma_l = j, Y \sim \mathcal{N}\Big(diag(\sigma_l^2)\nu_{j,l}, \; diag(\sigma_l^2)\Big)
\end{aligned}
$$

It can be verified that in the case where each variable is its own group, then these updates revert back to the original SuSiE updates.


# Applications
In this section, I walk through some of the more immediate applications of Group SuSiE.

## Categorical Variables
The motivation for thinking about Group SuSiE was to have regular SuSiE be able to handle categorical variables. For instance, suppose we have a variable with three categorical levels. Then in order to include these variables in our design matrix $X$, we need to use 2 vectors (plus the intercept). In order for the categorical variable as a whole to be included in our model, we need to be able to include both of these binary variables simultaneously. However, regular SuSiE does not allow us to achieve this. But if we put these two binary variables together into a group, then Group SuSiE can include/exclude the entire group simultaneously.

**CAUTION**

I believe that in this context, the design matrix should use the "sum contrast" rather than the "treatment contrast." Since we are placing a prior on the effects, then if we were to use the treatment contrast, our model would not be invariant to re-labelling of the baseline group (I think, since the baseline gets "absorbed" into the intercept, which is given a different prior). Instead, if we use the sum contrast, our model should be invariant to re-labelling.

For example, consider the case where we have a single categorical predictor with 3 levels. Suppose that the residual variance is very small, so the responses in each group are very close to their expectation. Suppose that the mean response for group 1 is 100, for group 2 is 200, and for group 3 is 300. Ideally, we want our model to be invariant to re-labelling the groups, since the group labels are arbitrary. If we were to use treatment coding for our variable, then the intercept would correspond to the group 1 mean. Since the intercept has a flat prior, then the estimate for the intercept will be very close to 100. The coefficient for group 2 (respectively, group 3) will correspond to the difference between the mean response in group 1 and group 2 (respectively, group 3). Since our prior for these effects has a smaller variance, the estimate for the group 2 coefficient will be shrunk from 100 ($100 = 200 - 100$) towards 0 (e.g. 50), and the estimate for the group 3 coefficient will be shrunk from 200 ($200 = 300 - 100$) towards 0 (e.g. 100). However, suppose we re-order the group labels so that now group 3 is the baseline. So the estimate for the intercept will be very close to 300, the estimate for the group 1 coefficient will be shrunk from -200 to 0 (e.g. -100), and the estimate for the group 2 coefficient will be shrunk from -100 to 0 (e.g. -50). Clearly, these two fitted models are different.

If, instead, we use the sum-to-zero constraint, then the intercept will correspond to the overall mean, which is invariant to re-labelling of the groups. I believe it will be the case that the coefficient for the groups (interpreted as the difference between the overall mean and the group mean) will also be invariant to re-lablling. However, I need to think a little mode (e.g. group 3 coefficient is the negative sum of the first and second coefficients, but if we re-label and estimate the group 3 coefficient normally, would we get the same thing?).

Alternatively, it might be best to instead represent a categorical predictor with $p_j$ levels with $p_j$ indicator variables denoting group membership. This will definitely be invariant to re-labelling of the groups. However, I'm not sure if/how that will act, since including 3 variables instead of the canonical 2 makes the model matrix rank deficient (due to the intercept column).

### Order-2 Trees
A direct application of including categorical predictors with 3 groups is to extend SuSiE Stumps to include trees of order 2. We can represent the tree as a categorical variable with 3 groups.


## Variables in a Tree Hierarchy
Another application of Group SuSiE could be data with predictors that are arranged in a hierarchy. Suppose the variables are arranged in a directed tree, where each node signifies that the variable[s] at that node is/are subtypes of the variables at the ancestors to that node. In this case, each node would correspond to a group, which would contain the variable[s] at that node plus all variables from ancestors of that node.

### Gene Set Enrichment (?)
A particular application of Group SuSiE with hierarchical variables could be gene set enrichment analysis. If we have pre-specified gene sets (e.g. GO terms), then we can group our variables (e.g. gene expression measurements) based on these gene sets.


## Polynomials of Variables
Another application could be to include higher-order terms of our variables (e.g. if our predictor is $x$, then we could include $x^2$, $x^3$, etc). As we learn in our standard linear regression classes, we should always include lower-order terms when we add higher-order terms. Thus, we can create groups of variables $G_j$ as:
$$
\begin{aligned}
G_1 := \{x\} \\
G_2 := \{x, \; x^2\} \\
G_3 := \{x, \; x^2, \; x^3\}
\end{aligned}
$$
etc.


# Questions/Issues
The section below outlines some of the more immediate questions that I have had.

## Correlations Between Variables Within a Group
In the context of gene set enrichment analysis specifically, we expect gene expression within a gene set to be highly correlated. I do not know if this method will be able to handle a group of highly correlated variables. Especially since once a group is "selected," there is no further penalty placed on the number of non-zero effects within the group.

## Regularizer on Group Size (Prior Probability vs. Prior Variance)
If the prior probability on each group being non-zero, $\pi_j$, is the same for all groups, and the prior variance of the effects, $\sigma_{0l}^2$, does not vary between groups, then if one group is a superset of another, the larger group will always have a higher posterior probability of being non-zero. This is a problem, especially for hierarchical groups (e.g. GO terms). My proposal to help remedy this is to place a lower prior probability on larger groups being non-zero.

In seeing the potential application to use SuSiE using a basis of trees of order 2, I took inspiration from the prior used in [BART: Bayesian Additive Regression Trees](https://projecteuclid.org/download/pdfview_1/euclid.aoas/1273584455). Here, they place a prior on the probability that a node of the tree at depth $d$ is non-terminal: $p(\text{node at depth } d \text{ is non-terminal}) = \alpha(1 + d)^{-\beta}, \quad \alpha \in (0, 1), \; \beta \in [0, \infty)$. There is a correspondence between the number of terminal nodes of a tree and the number of levels in a categorical variable required to represent that tree. As a result, I believe it is worth thinking about how to modify this component of the BART prior to choose default values of $\pi_j$ based on the number of variables in group $j$.

## Overlap Between Groups
Nowhere in the derivation of the updates does it require groups to be disjoint. However, I am not sure if any issues will arise if the groups have overlapping variables.

## Interpretation of Credible Sets
In regular SuSiE, the approximate posterior distribution leads to the easy interpretation of credible sets of variables. However, this interpretation is somewhat lost in Group SuSiE, since now we consider credible sets of groups of variables.
